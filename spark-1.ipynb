{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f126d3a-b44b-4ce9-84a1-31eb92fc0282",
   "metadata": {},
   "source": [
    "## Spark I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df371554-e828-4d5e-bc97-c8ddebf86073",
   "metadata": {},
   "source": [
    "Primeros pasos con Apache Spark\n",
    "\n",
    "\n",
    "**쯈u칠 es spark?**\n",
    "\n",
    "> Apache spark es un motor unificado de c치lculo en memoria y un conjunto de bibliotecas para procesamiento paralelo y distribuido de datos en cl칰steres de ordenadores\n",
    "> \n",
    "\n",
    "**쮺칩mo se puede usar Spark?** \n",
    "\n",
    "1. Desde un API de DataFrames\n",
    "2. Lanzando consultas SQL \n",
    "3. Desde una instrucci칩n de la API \n",
    "4. Desde un consulta de tarea de grafos \n",
    "\n",
    "**쯈u칠 es el echo de que sea en memoria?**\n",
    "\n",
    "> Todos los c치lculos se llevan en memoria y solo se escriben resultados en disco s칤 el usuario lo especifica o que la operaci칩n requiere mover datos entre nodos (shuffle)\n",
    "> \n",
    "\n",
    "**쯈u칠 ventajas tiene?**\n",
    "\n",
    "- Consiste en APi m치s intuitiva que MapReduce\n",
    "- Abstrae todos los detalles de comunicaci칩n en red.\n",
    "- Opera similar a transacciones SQL (tablas distribuidas)\n",
    "- Ofrece para  distintos lenguajes: Java, Scala y Python\n",
    "- Pyspark es el paquete de python\n",
    "\n",
    "**쮺칩mo se componen Spark?**\n",
    "![Untitled](./imgs/Untitled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e82de9-aed5-4490-8f99-d3d326e48286",
   "metadata": {},
   "source": [
    "\n",
    "- Spark core: compoenente principal, almacena estructuras de datos fundamentales para trabajar con spark e.g RDD\n",
    "- Los tres componentes  inferiores, permiten gestionar recursos del cl칰ster, maquinas, CPU y RAM\n",
    "- Spark SQL: API para manejar tablas distribuidas, estructuradas con nombre y tipo, DataFrames.\n",
    "- Spark Streaming: Procesamiento de flujos de datos en tiempo real\n",
    "- Spark MLlib: Implementaciones distribuidas de algoritmos ML\n",
    "- Spark GraphX: Representa un grafo como una pareja de DataFrames, con los nodos y los arcos.\n",
    "\n",
    "## Arquitectura de Spark\n",
    "\n",
    "<aside>\n",
    "游꿢 Al escribir  na aplicaci칩n en spark es una aplicaci칩n **secuencial.** No paralela, que es en una m치quina interna o externa al cl칰ster.  El programa driver puede conectarse al servidor de cl칰ster.\n",
    "\n",
    "</aside>\n",
    "\n",
    " \n",
    "\n",
    "![Untitled](imgs/Untitled%201.png)\n",
    "\n",
    "Al ejecutar usamos un sparkSession, \n",
    "\n",
    "**쯈u칠 es el sparkSession?**\n",
    "\n",
    "Es el encargado de establecer la conexi칩n con el gestor de cl칰ster para poder enviar tareas a los workers. \n",
    "\n",
    " Al crear un sparkSession es necesario:\n",
    "\n",
    "- Indicar el n칰mero de nodos\n",
    "- RAM\n",
    "- N칰mero de cores f칤sicos\n",
    "\n",
    "Esto para cada nodo. Estos recursos se llaman *executor,* \n",
    "\n",
    "**쯈u칠 es un *executor*?**\n",
    "\n",
    "<aside>\n",
    "游꿢 Un *executor: es un proceso de la JVM que se ejecuta en el nodo y ocupa los recursos establecidos en el sparkSession.*\n",
    "\n",
    "</aside>\n",
    "\n",
    "Un ejecutor es creado por el gestor del cl칰ster  cuando arranca nuestra aplicaci칩n de Spark y muere cuando la aplicaci칩n finaliza \n",
    "\n",
    "> Cada ejecutor queda preparado para ejecutar*tareas*맋e Spark, que es la unidad m칤nima de ejecuci칩n de trabajos. Cada tarea requiere un*core*맓ibre para ejecutarse, por lo que, si un*executor*맚iene reservados cuatro*cores*, podr치 ejecutar cuatro tareas en paralelo.\n",
    "> \n",
    "\n",
    "<aside>\n",
    "游꿢 Cuando el flujo del programa llega a ciertas funciones especificas de Spark que desencadenan ejecuci칩n distribuida.\n",
    "\n",
    "</aside>\n",
    "\n",
    "### Resilient Distributed Datasets\n",
    "\n",
    "쯈u칠 son los RDD?\n",
    "\n",
    "- Es una abstracci칩n fundamental de Spark\n",
    "- Es una colecci칩n no ordenada de objetos (dag)\n",
    "- Vive en la RAM de los Nodos de un cl칰ster\n",
    "\n",
    "Las colecciones est치 dividida en particiones, las cuales est치n en la RAM de los nodos \n",
    "\n",
    "1. Resilient: Resistente, Adaptable. Es posible reconstruir un RDD usando los DAG \n",
    "2. Distributed: Particiones de est치n distribuidos en la RAM de los nodos, al no estar ordenado no es posible acceder por indice.  \n",
    "3. Dataset: La colecci칩n representa un conjunto de datos que estamos procesando de forma paralela. \n",
    "\n",
    "<aside>\n",
    "丘멆잺 **no hay replicaci칩n de cada partici칩n**.\n",
    "\n",
    "</aside>\n",
    "\n",
    "![Untitled](imgs/Untitled%202.png)\n",
    "\n",
    "es posible reconstruir las particiones que estuvieran en ese momento en su memoria principal gracias al DAG. \n",
    "\n",
    "**쯈u칠 es la inmutabilidad?**\n",
    "\n",
    "Un RDD no puede modificarse una vez creado. Lo que hacemos es aplicar transformaciones a los RDD para obtener otros datos.  Las peticiones se ejecutan en paralelo sobre todas las paticiones del RDD de manera transparente, lo que da lugar a un nuevo RDD. \n",
    "\n",
    "> **Ejemplo**: dado un RDD de n칰meros reales, para multiplicar cada elemento por\n",
    "dos, aplicamos una transformaci칩n que act칰a en cada elemento y lo multiplica\n",
    "por dos. Spark lleva nuestro c칩digo de la transformaci칩n (lo serializa y lo env칤a\n",
    "por la red) a cada uno de los nodos del cl칰ster donde haya particiones de ese\n",
    "RDD y lo ejecuta en ellos para que act칰e en cada elemento de esa partici칩n.\n",
    "Todo de manera transparente al programador.\n",
    "> \n",
    "\n",
    "### Partici칩n\n",
    "\n",
    "- Una partici칩n es un subconjunto de los objetos presentes en un RDD que residen en un mismo nodo en un cl칰ster distribuido.\n",
    "- Las particiones son la unidad de datos m칤nima en la cual se ejecuta una tarea de transformaci칩n de manera independiente a las dem치s particiones.\n",
    "- Es recomendable tener al menos tantas particiones como cores f칤sicos (procesadores) disponibles en el cl칰ster para aprovechar al m치ximo los recursos de hardware.\n",
    "- Idealmente, cada core deber칤a tener una partici칩n asignada para garantizar que todos los cores est칠n ocupados incluso si solo se est치 utilizando la aplicaci칩n en el cl칰ster.\n",
    "- Lo habitual es tener muchas particiones de un mismo RDD en cada nodo, en un n칰mero superior al de procesadores disponibles en el nodo.\n",
    "- Se recomienda que cada RDD est칠 dividido en un n칰mero de particiones que sea entre el doble y el triple del n칰mero de procesadores del cl칰ster. Esto permite una mejor distribuci칩n de la carga de trabajo y aprovechamiento de los recursos.\n",
    "\n",
    "<aside>\n",
    "丘멆잺 Es recomendable usar DataFrames en lugar de RDD, el motor de spark llamado Catalyst hace optimizaciones en el c칩digo de los DataFrames\n",
    "\n",
    "</aside>\n",
    "\n",
    "### Transformaciones y acciones\n",
    "\n",
    "**쯈u칠 son las transformaciones?**\n",
    "\n",
    "Las transformaciones se ejecutan en un RDD que genera uno nuevo, son ******lazy,****** lo que hace spark es agregar la transformaci칩n al DAG y ejecutarla hasta que encuentre una acci칩n. \n",
    "\n",
    "<aside>\n",
    "丘멆잺 El **DAG** es el grao que permite hacer la trazabilidad de las operaciones y tener la *resiliency*\n",
    "\n",
    "</aside>\n",
    "\n",
    "- S칤 la transformaci칩n no requiere un shuffle (movimiento de datos), es llamado **************narrow************** y\n",
    "\n",
    "**쯈u칠 son las acciones?**\n",
    "\n",
    "Recibe un RDD y calcula un resultado (integers, doubles etc) y lo devuelve al driver (programa principal)\n",
    "\n",
    "<aside>\n",
    "丘멆잺 El tipo de dato devuelto NO ES UN RDD, es un tipo nativo del lenguaje que estemos usando.\n",
    "\n",
    "</aside>\n",
    "\n",
    "Algunas puntos importantes de las acciones:\n",
    "\n",
    "- El resultado cae en la memoria de la maquina driver\n",
    "- Una acci칩n desencadena la ejecuci칩n de las transformaciones intermedias y la materializaci칩n de los nuevos RDD\n",
    "- Una vez creados los RDD se aplican las transformaciones que toque, seg칰n indica el DAG\n",
    "- Se libera el RDD (no permanecen en memoria RAM )\n",
    "- Un RDD cacheado permanece materializado en la RAM de los nodos y no es necesario recalcularlo despu칠s de que se haya materializado la primera vez.\n",
    "- Cachear un RDD ayuda a no reconstruirlo, la reconstrucci칩n de un RDD comienza con el DAG, acceso a los datos  y aplicar las transformaciones.\n",
    "\n",
    "### Transformaciones m치s habituales\n",
    "\n",
    "1. map: recibe una funci칩n que se ejecuta a cada elemento del RDD, retorna un nuevo RDD con los elementos transformados.\n",
    "2. flatMap; recibe una funci칩n que se ejecuta por cada uno de los elementos y regresa un vector de valroes para cada elemento\n",
    "3. filter: Filtra un RDD, regresa True si el elemento debe ser incluido en el nuevo RDD\n",
    "4. sample: Genera una muestra aleatoria del RDD de tama침o especifica  \n",
    "5. union: La uni칩n de dos RDD parados como par치metro \n",
    "6. intersection: devuelve la intersecci칩n de dos RDD, los elementos que est치n presentes en ambos \n",
    "7. distinct: Quita los elementos repetidos  \n",
    "\n",
    "**Transformaciones espec칤ficas para un PairRDD, es decir, RDD de pares (clave, valor):**\n",
    "\n",
    "1. groupByKey: Se agrupan los elementos del RDD seg칰n su clave. Todos los elementos con la misma clave se agrupan juntos.\n",
    "2. reduceByKey:  Se agrupan los elementos por clave, se pasa una funci칩n que recibe dos par치metros y retornar uno. \n",
    "3. sortByKey: Ordena los elementos del RDD por clave \n",
    "4. join: Combina dos RDD de tal modo que se junten los elementos que tienen la\n",
    "misma clave.\n",
    "\n",
    "<aside>\n",
    "丘멆잺 En resumen, la propiedad conmutativa implica que el orden de las operaciones no afecta el resultado, mientras que la propiedad asociativa implica que el resultado es independiente de c칩mo se agrupen los operandos o los valores intermedios. Ambas propiedades son fundamentales en el procesamiento de RDDs y garantizan resultados consistentes y predecibles en operaciones algebraicas.\n",
    "\n",
    "</aside>\n",
    "\n",
    "### Acciones m치s habituales en RDD\n",
    "\n",
    "<aside>\n",
    "游꿢 Todas las acciones llevan resultados a los drivers. por lo que estos tienen que caber en la memoria del proceso ******driver******\n",
    "\n",
    "</aside>\n",
    "\n",
    "1. reduce: \n",
    "2. collect: devuelve todos los elementos contenidos en el RDD como una colecci칩n, lanza una ecepci칩n cuando la lista es muy grande y no cabe en la RAM\n",
    "3. count: N칰mero de elementos contenidos en el RDD\n",
    "4. take: Devuelve los n primeros elementos. No hay garant칤as de ordenaci칩n. \n",
    "5. first: Retorna el primer elemento del RDD\n",
    "6. takeSample: Devuelve n elemento aleatorios del RDD\n",
    "7. takeOrdered: devuelve los n primeros elementos del RDD tras haber realizado\n",
    "una ordenaci칩n de todos los elementos contenidos en el mismo.\n",
    "8. countByKey: cuenta el n칰mero de elementos en el RDD para cada clave\n",
    "diferente\n",
    "9. saveAsTextFile: : guarda los contenidos del RDD en un fichero de texto.\n",
    "\n",
    "![Untitled](imgs/Untitled%203.png)\n",
    "\n",
    "```python\n",
    "func_multiplicar = lambda x: (x, 3*x) # funci칩n que devuelve una tupla\n",
    "A = sc.parallelize([5.0, 3.2, 1.1, -2.4, # distribuimos la lista como\n",
    " 8.9, 4.4, 3.7, 9.1], 3) # un RDD de 3 particiones\n",
    "# sc se refiere a un SparkContext que realiza la conexi칩n \n",
    "# con el gestor de cl칰ster.\n",
    "\n",
    "B = A.map(func_multiplicar)\n",
    "B = B.reduceByKey(lambda v1, v2: v1+v2)\n",
    "C = sc.parallelize([(5.0, 1.0), (1.1, -3)], 4) # lista a RDD de 4 part\n",
    "D = C.map(lambda tuple: (tuple[0], 2*tuple[1]))\n",
    "E = D.filter(lambda tuple: tuple[1] > 1)\n",
    "F = E.join(B)\n",
    "resultado = F.take(3)\n",
    "\n",
    "```\n",
    "\n",
    "### Jobs, stages y tasks\n",
    "\n",
    "<aside>\n",
    "游꿢 Un job (trabajo) de Spark es todo el procesamiento necesario para llevar a\n",
    "cabo una acci칩n del usuario.\n",
    "\n",
    "</aside>\n",
    "\n",
    "<aside>\n",
    "游꿢 Un stage es todo el procesamiento que puede llevarse a cabo sin mover datos\n",
    "entre nodos\n",
    "\n",
    "</aside>\n",
    "\n",
    "<aside>\n",
    "游꿢 Una tarea de Spark es el procesamiento aplicado por un core f칤sico (CPU) a\n",
    "una partici칩n de un RDD.\n",
    "\n",
    "</aside>\n",
    "\n",
    "![Untitled](imgs/Untitled%204.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d96e0-beb4-4077-b24e-9fe584bba9e9",
   "metadata": {},
   "source": [
    "### Ejemplo de Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feb19c-3a8c-4ee2-bac4-cc480611711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deef4c5e-114a-45f5-bc5a-8fb8a13d0566",
   "metadata": {},
   "source": [
    "Crear una aplicaci칩n de "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2ed1c4-3b9a-47d8-9340-f6ec5df72c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark_1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4883ac01-7a7b-4225-82db-5d76662593a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb42e3-7e2f-40ca-9767-57f82d355713",
   "metadata": {},
   "source": [
    "En Spark existe diferentes formas de leer un archivo, aqu칤 se muestra una forma \"artesanal\" de procesar un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68eda31-7f45-46cf-8f6e-6db9d3b624e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sparkContext.textFile('./data/flights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c01f37d-ac2c-4cf4-bceb-f77d1b0e5e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_line = flights.first()\n",
    "flights_lines = flights.filter(lambda line: line != header_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3bb89c-f4bd-40b9-a0cd-aae58f09028c",
   "metadata": {},
   "source": [
    "Leer la primera l칤nea para despu칠s filtrar el resto del archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a60e3-ea6e-4d14-b59d-1769d15bca3d",
   "metadata": {},
   "source": [
    "En Spark existen dos terminos importantes:\n",
    "- Acciones: Recibe un RDD y calcula un resultado (integers, doubles etc) y lo devuelve al driver (programa principal)\n",
    "- Transformaciones:Las transformaciones se ejecutan en un RDD que genera uno nuevo, son lazy, lo que hace spark es agregar la transformaci칩n al DAG y ejecutarla hasta que encuentre una acci칩n. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa07bb-ee76-4745-97b6-8be64791d3d7",
   "metadata": {},
   "source": [
    "flights_lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd4a4d-164f-482a-b4bc-6c0fcf398434",
   "metadata": {},
   "source": [
    "<code>take(n)</code> es una acci칩n que devuelve los n primeros elementos. No hay garant칤as de ordenaci칩n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993855c-28c1-423d-a542-47f4ab333607",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_rdd = flights_lines.map(lambda line: line.split(',')).map(lambda fields_list:           \n",
    "                                     (int(fields_list[0]),\n",
    "                                      int(fields_list[1]),\n",
    "                                      int(fields_list[2]),\n",
    "                                      fields_list[3],\n",
    "                                      fields_list[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31959325-41fb-4ea0-8686-780a078cd7f6",
   "metadata": {},
   "source": [
    "El c칩digo anterior regresa un nuevo RDD con los campos \"convertidos\" a integers y strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fd1ab5f-46d7-477f-8ee2-95559ef68330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2013, 1, 1, 'EWR', 'IAH'),\n",
       " (2013, 1, 1, 'LGA', 'IAH'),\n",
       " (2013, 1, 1, 'JFK', 'MIA'),\n",
       " (2013, 1, 1, 'JFK', 'BQN'),\n",
       " (2013, 1, 1, 'LGA', 'ATL'),\n",
       " (2013, 1, 1, 'EWR', 'ORD'),\n",
       " (2013, 1, 1, 'EWR', 'FLL'),\n",
       " (2013, 1, 1, 'LGA', 'IAD'),\n",
       " (2013, 1, 1, 'JFK', 'MCO'),\n",
       " (2013, 1, 1, 'LGA', 'ORD')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee8d8e0-d1c1-4d2a-bb81-2cc4209892e0",
   "metadata": {},
   "source": [
    "La transformaci칩n <code>map(function)</code>, recibe una funci칩n que se ejecuta a cada elemento del RDD, retorna un nuevo RDD con los elementos transformados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad86b5-3f41-48c9-ab01-1deed080dfae",
   "metadata": {},
   "source": [
    "En la siguiente funci칩n lambda crea un nuevo PairRDD clave-valor,  cuya pisici칩n 0 de la tupla es la clave y el valor es posici칩n 1 de la tupla. \n",
    "\n",
    "\n",
    "En este caso se crea una tupla con la clave de destino con valor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc0a42d-9fab-42d2-9010-19cc83a859db",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_rdd_count = flights_rdd.map(lambda line: (line[4],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84668bcd-74b9-4694-9857-53682b9f00e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IAH', 1), ('IAH', 1), ('MIA', 1), ('BQN', 1), ('ATL', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_rdd_count.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b0abc-33ef-48f5-901e-099ca7499281",
   "metadata": {},
   "source": [
    "Para contar cu치ntos vuelos tenie como destino cada uno de los destinos en el fichero flights.csv Para ello, podemos utilizar el pairRDD previo, agrupar por destino y sumar los valores de cada clave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87747e9a-9e07-446f-a5e5-e3d296501be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_dest_rdd_groupped = flights_rdd_count.groupByKey().map(\n",
    "    lambda line: (line[0],sum( line[1] ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b1c76df-11e7-44aa-af4c-6a493a99a6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IAH', 7198),\n",
       " ('MIA', 11728),\n",
       " ('BQN', 896),\n",
       " ('ORD', 17283),\n",
       " ('IAD', 5700),\n",
       " ('MCO', 14082),\n",
       " ('TPA', 7466),\n",
       " ('DFW', 8738),\n",
       " ('BOS', 15508),\n",
       " ('MSP', 7185)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_dest_rdd_groupped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c17196-deae-4794-9623-e3117fa31ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
