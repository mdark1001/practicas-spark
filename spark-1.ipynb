{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f126d3a-b44b-4ce9-84a1-31eb92fc0282",
   "metadata": {},
   "source": [
    "## Spark I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df371554-e828-4d5e-bc97-c8ddebf86073",
   "metadata": {},
   "source": [
    "Primeros pasos con Apache Spark\n",
    "\n",
    "\n",
    "**¿Qué es spark?**\n",
    "\n",
    "> Apache spark es un motor unificado de cálculo en memoria y un conjunto de bibliotecas para procesamiento paralelo y distribuido de datos en clústeres de ordenadores\n",
    "> \n",
    "\n",
    "**¿Cómo se puede usar Spark?** \n",
    "\n",
    "1. Desde un API de DataFrames\n",
    "2. Lanzando consultas SQL \n",
    "3. Desde una instrucción de la API \n",
    "4. Desde un consulta de tarea de grafos \n",
    "\n",
    "**¿Qué es el echo de que sea en memoria?**\n",
    "\n",
    "> Todos los cálculos se llevan en memoria y solo se escriben resultados en disco sí el usuario lo especifica o que la operación requiere mover datos entre nodos (shuffle)\n",
    "> \n",
    "\n",
    "**¿Qué ventajas tiene?**\n",
    "\n",
    "- Consiste en APi más intuitiva que MapReduce\n",
    "- Abstrae todos los detalles de comunicación en red.\n",
    "- Opera similar a transacciones SQL (tablas distribuidas)\n",
    "- Ofrece para  distintos lenguajes: Java, Scala y Python\n",
    "- Pyspark es el paquete de python\n",
    "\n",
    "**¿Cómo se componen Spark?**\n",
    "![Untitled](./imgs/Untitled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e82de9-aed5-4490-8f99-d3d326e48286",
   "metadata": {},
   "source": [
    "\n",
    "- Spark core: compoenente principal, almacena estructuras de datos fundamentales para trabajar con spark e.g RDD\n",
    "- Los tres componentes  inferiores, permiten gestionar recursos del clúster, maquinas, CPU y RAM\n",
    "- Spark SQL: API para manejar tablas distribuidas, estructuradas con nombre y tipo, DataFrames.\n",
    "- Spark Streaming: Procesamiento de flujos de datos en tiempo real\n",
    "- Spark MLlib: Implementaciones distribuidas de algoritmos ML\n",
    "- Spark GraphX: Representa un grafo como una pareja de DataFrames, con los nodos y los arcos.\n",
    "\n",
    "## Arquitectura de Spark\n",
    "\n",
    "<aside>\n",
    "🎯 Al escribir  na aplicación en spark es una aplicación **secuencial.** No paralela, que es en una máquina interna o externa al clúster.  El programa driver puede conectarse al servidor de clúster.\n",
    "\n",
    "</aside>\n",
    "\n",
    " \n",
    "\n",
    "![Untitled](imgs/Untitled%201.png)\n",
    "\n",
    "Al ejecutar usamos un sparkSession, \n",
    "\n",
    "**¿Qué es el sparkSession?**\n",
    "\n",
    "Es el encargado de establecer la conexión con el gestor de clúster para poder enviar tareas a los workers. \n",
    "\n",
    " Al crear un sparkSession es necesario:\n",
    "\n",
    "- Indicar el número de nodos\n",
    "- RAM\n",
    "- Número de cores físicos\n",
    "\n",
    "Esto para cada nodo. Estos recursos se llaman *executor,* \n",
    "\n",
    "**¿Qué es un *executor*?**\n",
    "\n",
    "<aside>\n",
    "🎯 Un *executor: es un proceso de la JVM que se ejecuta en el nodo y ocupa los recursos establecidos en el sparkSession.*\n",
    "\n",
    "</aside>\n",
    "\n",
    "Un ejecutor es creado por el gestor del clúster  cuando arranca nuestra aplicación de Spark y muere cuando la aplicación finaliza \n",
    "\n",
    "> Cada ejecutor queda preparado para ejecutar *tareas* de Spark, que es la unidad mínima de ejecución de trabajos. Cada tarea requiere un *core* libre para ejecutarse, por lo que, si un *executor* tiene reservados cuatro *cores*, podrá ejecutar cuatro tareas en paralelo.\n",
    "> \n",
    "\n",
    "<aside>\n",
    "🎯 Cuando el flujo del programa llega a ciertas funciones especificas de Spark que desencadenan ejecución distribuida.\n",
    "\n",
    "</aside>\n",
    "\n",
    "### Resilient Distributed Datasets\n",
    "\n",
    "¿Qué son los RDD?\n",
    "\n",
    "- Es una abstracción fundamental de Spark\n",
    "- Es una colección no ordenada de objetos (dag)\n",
    "- Vive en la RAM de los Nodos de un clúster\n",
    "\n",
    "Las colecciones está dividida en particiones, las cuales están en la RAM de los nodos \n",
    "\n",
    "1. Resilient: Resistente, Adaptable. Es posible reconstruir un RDD usando los DAG \n",
    "2. Distributed: Particiones de están distribuidos en la RAM de los nodos, al no estar ordenado no es posible acceder por indice.  \n",
    "3. Dataset: La colección representa un conjunto de datos que estamos procesando de forma paralela. \n",
    "\n",
    "<aside>\n",
    "⚠️ **no hay replicación de cada partición**.\n",
    "\n",
    "</aside>\n",
    "\n",
    "![Untitled](imgs/Untitled%202.png)\n",
    "\n",
    "es posible reconstruir las particiones que estuvieran en ese momento en su memoria principal gracias al DAG. \n",
    "\n",
    "**¿Qué es la inmutabilidad?**\n",
    "\n",
    "Un RDD no puede modificarse una vez creado. Lo que hacemos es aplicar transformaciones a los RDD para obtener otros datos.  Las peticiones se ejecutan en paralelo sobre todas las paticiones del RDD de manera transparente, lo que da lugar a un nuevo RDD. \n",
    "\n",
    "> **Ejemplo**: dado un RDD de números reales, para multiplicar cada elemento por\n",
    "dos, aplicamos una transformación que actúa en cada elemento y lo multiplica\n",
    "por dos. Spark lleva nuestro código de la transformación (lo serializa y lo envía\n",
    "por la red) a cada uno de los nodos del clúster donde haya particiones de ese\n",
    "RDD y lo ejecuta en ellos para que actúe en cada elemento de esa partición.\n",
    "Todo de manera transparente al programador.\n",
    "> \n",
    "\n",
    "### Partición\n",
    "\n",
    "- Una partición es un subconjunto de los objetos presentes en un RDD que residen en un mismo nodo en un clúster distribuido.\n",
    "- Las particiones son la unidad de datos mínima en la cual se ejecuta una tarea de transformación de manera independiente a las demás particiones.\n",
    "- Es recomendable tener al menos tantas particiones como cores físicos (procesadores) disponibles en el clúster para aprovechar al máximo los recursos de hardware.\n",
    "- Idealmente, cada core debería tener una partición asignada para garantizar que todos los cores estén ocupados incluso si solo se está utilizando la aplicación en el clúster.\n",
    "- Lo habitual es tener muchas particiones de un mismo RDD en cada nodo, en un número superior al de procesadores disponibles en el nodo.\n",
    "- Se recomienda que cada RDD esté dividido en un número de particiones que sea entre el doble y el triple del número de procesadores del clúster. Esto permite una mejor distribución de la carga de trabajo y aprovechamiento de los recursos.\n",
    "\n",
    "<aside>\n",
    "⚠️ Es recomendable usar DataFrames en lugar de RDD, el motor de spark llamado Catalyst hace optimizaciones en el código de los DataFrames\n",
    "\n",
    "</aside>\n",
    "\n",
    "### Transformaciones y acciones\n",
    "\n",
    "**¿Qué son las transformaciones?**\n",
    "\n",
    "Las transformaciones se ejecutan en un RDD que genera uno nuevo, son ******lazy,****** lo que hace spark es agregar la transformación al DAG y ejecutarla hasta que encuentre una acción. \n",
    "\n",
    "<aside>\n",
    "⚠️ El **DAG** es el grao que permite hacer la trazabilidad de las operaciones y tener la *resiliency*\n",
    "\n",
    "</aside>\n",
    "\n",
    "- Sí la transformación no requiere un shuffle (movimiento de datos), es llamado **************narrow************** y\n",
    "\n",
    "**¿Qué son las acciones?**\n",
    "\n",
    "Recibe un RDD y calcula un resultado (integers, doubles etc) y lo devuelve al driver (programa principal)\n",
    "\n",
    "<aside>\n",
    "⚠️ El tipo de dato devuelto NO ES UN RDD, es un tipo nativo del lenguaje que estemos usando.\n",
    "\n",
    "</aside>\n",
    "\n",
    "Algunas puntos importantes de las acciones:\n",
    "\n",
    "- El resultado cae en la memoria de la maquina driver\n",
    "- Una acción desencadena la ejecución de las transformaciones intermedias y la materialización de los nuevos RDD\n",
    "- Una vez creados los RDD se aplican las transformaciones que toque, según indica el DAG\n",
    "- Se libera el RDD (no permanecen en memoria RAM )\n",
    "- Un RDD cacheado permanece materializado en la RAM de los nodos y no es necesario recalcularlo después de que se haya materializado la primera vez.\n",
    "- Cachear un RDD ayuda a no reconstruirlo, la reconstrucción de un RDD comienza con el DAG, acceso a los datos  y aplicar las transformaciones.\n",
    "\n",
    "### Transformaciones más habituales\n",
    "\n",
    "1. map: recibe una función que se ejecuta a cada elemento del RDD, retorna un nuevo RDD con los elementos transformados.\n",
    "2. flatMap; recibe una función que se ejecuta por cada uno de los elementos y regresa un vector de valroes para cada elemento\n",
    "3. filter: Filtra un RDD, regresa True si el elemento debe ser incluido en el nuevo RDD\n",
    "4. sample: Genera una muestra aleatoria del RDD de tamaño especifica  \n",
    "5. union: La unión de dos RDD parados como parámetro \n",
    "6. intersection: devuelve la intersección de dos RDD, los elementos que están presentes en ambos \n",
    "7. distinct: Quita los elementos repetidos  \n",
    "\n",
    "**Transformaciones específicas para un PairRDD, es decir, RDD de pares (clave, valor):**\n",
    "\n",
    "1. groupByKey: Se agrupan los elementos del RDD según su clave. Todos los elementos con la misma clave se agrupan juntos.\n",
    "2. reduceByKey:  Se agrupan los elementos por clave, se pasa una función que recibe dos parámetros y retornar uno. \n",
    "3. sortByKey: Ordena los elementos del RDD por clave \n",
    "4. join: Combina dos RDD de tal modo que se junten los elementos que tienen la\n",
    "misma clave.\n",
    "\n",
    "<aside>\n",
    "⚠️ En resumen, la propiedad conmutativa implica que el orden de las operaciones no afecta el resultado, mientras que la propiedad asociativa implica que el resultado es independiente de cómo se agrupen los operandos o los valores intermedios. Ambas propiedades son fundamentales en el procesamiento de RDDs y garantizan resultados consistentes y predecibles en operaciones algebraicas.\n",
    "\n",
    "</aside>\n",
    "\n",
    "### Acciones más habituales en RDD\n",
    "\n",
    "<aside>\n",
    "🎯 Todas las acciones llevan resultados a los drivers. por lo que estos tienen que caber en la memoria del proceso ******driver******\n",
    "\n",
    "</aside>\n",
    "\n",
    "1. reduce: \n",
    "2. collect: devuelve todos los elementos contenidos en el RDD como una colección, lanza una ecepción cuando la lista es muy grande y no cabe en la RAM\n",
    "3. count: Número de elementos contenidos en el RDD\n",
    "4. take: Devuelve los n primeros elementos. No hay garantías de ordenación. \n",
    "5. first: Retorna el primer elemento del RDD\n",
    "6. takeSample: Devuelve n elemento aleatorios del RDD\n",
    "7. takeOrdered: devuelve los n primeros elementos del RDD tras haber realizado\n",
    "una ordenación de todos los elementos contenidos en el mismo.\n",
    "8. countByKey: cuenta el número de elementos en el RDD para cada clave\n",
    "diferente\n",
    "9. saveAsTextFile: : guarda los contenidos del RDD en un fichero de texto.\n",
    "\n",
    "![Untitled](imgs/Untitled%203.png)\n",
    "\n",
    "```python\n",
    "func_multiplicar = lambda x: (x, 3*x) # función que devuelve una tupla\n",
    "A = sc.parallelize([5.0, 3.2, 1.1, -2.4, # distribuimos la lista como\n",
    " 8.9, 4.4, 3.7, 9.1], 3) # un RDD de 3 particiones\n",
    "# sc se refiere a un SparkContext que realiza la conexión \n",
    "# con el gestor de clúster.\n",
    "\n",
    "B = A.map(func_multiplicar)\n",
    "B = B.reduceByKey(lambda v1, v2: v1+v2)\n",
    "C = sc.parallelize([(5.0, 1.0), (1.1, -3)], 4) # lista a RDD de 4 part\n",
    "D = C.map(lambda tuple: (tuple[0], 2*tuple[1]))\n",
    "E = D.filter(lambda tuple: tuple[1] > 1)\n",
    "F = E.join(B)\n",
    "resultado = F.take(3)\n",
    "\n",
    "```\n",
    "\n",
    "### Jobs, stages y tasks\n",
    "\n",
    "<aside>\n",
    "🎯 Un job (trabajo) de Spark es todo el procesamiento necesario para llevar a\n",
    "cabo una acción del usuario.\n",
    "\n",
    "</aside>\n",
    "\n",
    "<aside>\n",
    "🎯 Un stage es todo el procesamiento que puede llevarse a cabo sin mover datos\n",
    "entre nodos\n",
    "\n",
    "</aside>\n",
    "\n",
    "<aside>\n",
    "🎯 Una tarea de Spark es el procesamiento aplicado por un core físico (CPU) a\n",
    "una partición de un RDD.\n",
    "\n",
    "</aside>\n",
    "\n",
    "![Untitled](imgs/Untitled%204.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d96e0-beb4-4077-b24e-9fe584bba9e9",
   "metadata": {},
   "source": [
    "### Ejemplo de Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feb19c-3a8c-4ee2-bac4-cc480611711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deef4c5e-114a-45f5-bc5a-8fb8a13d0566",
   "metadata": {},
   "source": [
    "Crear una aplicación de "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2ed1c4-3b9a-47d8-9340-f6ec5df72c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark_1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4883ac01-7a7b-4225-82db-5d76662593a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb42e3-7e2f-40ca-9767-57f82d355713",
   "metadata": {},
   "source": [
    "En Spark existe diferentes formas de leer un archivo, aquí se muestra una forma \"artesanal\" de procesar un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68eda31-7f45-46cf-8f6e-6db9d3b624e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sparkContext.textFile('./data/flights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c01f37d-ac2c-4cf4-bceb-f77d1b0e5e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_line = flights.first()\n",
    "flights_lines = flights.filter(lambda line: line != header_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3bb89c-f4bd-40b9-a0cd-aae58f09028c",
   "metadata": {},
   "source": [
    "Leer la primera línea para después filtrar el resto del archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a60e3-ea6e-4d14-b59d-1769d15bca3d",
   "metadata": {},
   "source": [
    "En Spark existen dos terminos importantes:\n",
    "- Acciones: Recibe un RDD y calcula un resultado (integers, doubles etc) y lo devuelve al driver (programa principal)\n",
    "- Transformaciones:Las transformaciones se ejecutan en un RDD que genera uno nuevo, son lazy, lo que hace spark es agregar la transformación al DAG y ejecutarla hasta que encuentre una acción. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa07bb-ee76-4745-97b6-8be64791d3d7",
   "metadata": {},
   "source": [
    "flights_lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd4a4d-164f-482a-b4bc-6c0fcf398434",
   "metadata": {},
   "source": [
    "<code>take(n)</code> es una acción que devuelve los n primeros elementos. No hay garantías de ordenación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993855c-28c1-423d-a542-47f4ab333607",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_rdd = flights_lines.map(lambda line: line.split(',')).map(lambda fields_list:           \n",
    "                                     (int(fields_list[0]),\n",
    "                                      int(fields_list[1]),\n",
    "                                      int(fields_list[2]),\n",
    "                                      fields_list[3],\n",
    "                                      fields_list[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31959325-41fb-4ea0-8686-780a078cd7f6",
   "metadata": {},
   "source": [
    "El código anterior regresa un nuevo RDD con los campos \"convertidos\" a integers y strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fd1ab5f-46d7-477f-8ee2-95559ef68330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2013, 1, 1, 'EWR', 'IAH'),\n",
       " (2013, 1, 1, 'LGA', 'IAH'),\n",
       " (2013, 1, 1, 'JFK', 'MIA'),\n",
       " (2013, 1, 1, 'JFK', 'BQN'),\n",
       " (2013, 1, 1, 'LGA', 'ATL'),\n",
       " (2013, 1, 1, 'EWR', 'ORD'),\n",
       " (2013, 1, 1, 'EWR', 'FLL'),\n",
       " (2013, 1, 1, 'LGA', 'IAD'),\n",
       " (2013, 1, 1, 'JFK', 'MCO'),\n",
       " (2013, 1, 1, 'LGA', 'ORD')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee8d8e0-d1c1-4d2a-bb81-2cc4209892e0",
   "metadata": {},
   "source": [
    "La transformación <code>map(function)</code>, recibe una función que se ejecuta a cada elemento del RDD, retorna un nuevo RDD con los elementos transformados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad86b5-3f41-48c9-ab01-1deed080dfae",
   "metadata": {},
   "source": [
    "En la siguiente función lambda crea un nuevo PairRDD clave-valor,  cuya pisición 0 de la tupla es la clave y el valor es posición 1 de la tupla. \n",
    "\n",
    "\n",
    "En este caso se crea una tupla con la clave de destino con valor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc0a42d-9fab-42d2-9010-19cc83a859db",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_rdd_count = flights_rdd.map(lambda line: (line[4],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84668bcd-74b9-4694-9857-53682b9f00e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IAH', 1), ('IAH', 1), ('MIA', 1), ('BQN', 1), ('ATL', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_rdd_count.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b0abc-33ef-48f5-901e-099ca7499281",
   "metadata": {},
   "source": [
    "Para contar cuántos vuelos tenie como destino cada uno de los destinos en el fichero flights.csv Para ello, podemos utilizar el pairRDD previo, agrupar por destino y sumar los valores de cada clave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87747e9a-9e07-446f-a5e5-e3d296501be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_dest_rdd_groupped = flights_rdd_count.groupByKey().map(\n",
    "    lambda line: (line[0],sum( line[1] ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b1c76df-11e7-44aa-af4c-6a493a99a6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IAH', 7198),\n",
       " ('MIA', 11728),\n",
       " ('BQN', 896),\n",
       " ('ORD', 17283),\n",
       " ('IAD', 5700),\n",
       " ('MCO', 14082),\n",
       " ('TPA', 7466),\n",
       " ('DFW', 8738),\n",
       " ('BOS', 15508),\n",
       " ('MSP', 7185)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_dest_rdd_groupped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c17196-deae-4794-9623-e3117fa31ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
